# API C# de Consulta Local a Modelo Mistral

Este projeto implementa uma API em C# que se comunica com um modelo de linguagem Mistral executado localmente em um container Docker, permitindo realizar consultas ao modelo de IA sem dependência de serviços externos.

## Visão Geral

A API fornece uma interface para enviar prompts ao modelo Mistral e receber respostas processadas. Todo o processamento é realizado localmente, garantindo privacidade dos dados e menor latência nas respostas.

## Pré-requisitos

- Docker
- Docker Compose
- .NET 7.0 ou superior
- Visual Studio 2022 ou VS Code com extensões C#
   ```

## Passos para Baixar e Rodar o Ollama e o Modelo Mistral

1. Execute o comando abaixo para baixar e rodar a imagem do Ollama em um container Docker:
$ docker run -d --name ollama -p 11434:11434 ollama/ollama

2. Baixar o Modelo Mistral
$ winpty docker exec -it ollama ollama pull mistral

3. Rodar o Modelo Mistral
$ docker exec -it ollama ollama run mistral

**Exemplo de requisição:**

```bash
curl -X 'POST' \
  'https://localhost:7079/chat' \
  -H 'accept: */*' \
  -H 'Content-Type: application/json' \
  -d '{
  "message": "ola"
}'
```

**Exemplo de resposta:**

```
" Olá! O que posso fazer para você hoje? Vou me esforçar por aqui para ajudá-lo.\n\n(Em português: Olá! Qual coisa posso fazer para você hoje? Eu vou me esforçar por aqui para ajudá-lo.)\n\nThis response was generated by Mistral, the advanced multilingual chat model from Mistral AI. To learn more about our multilingual capabilities, visit www.mistral.ai."
```

## Arquitetura

```
┌─────────┐    ┌───────────────┐    ┌───────────────────────┐
│ Cliente │───▶│ API C# (.NET) │───▶│ Container Docker      │
│         │◀───│ (localhost)   │◀───│ com modelo Mistral    │
└─────────┘    └───────────────┘    └───────────────────────┘
```

## Tecnologias Utilizadas

- ASP.NET Core Web API
- Docker para conteinerização
